{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkvzZC3F-JfG",
        "outputId": "20f879f1-133f-424b-c1f9-a7dfaa0d463f"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "\n",
        "# For deep learning\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.ops import box_iou\n",
        "\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# For augmentations\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# Own package imports\n",
        "os.chdir('/home/naro/projects/Rumex')\n",
        "\n",
        "from config.paths_config import *\n",
        "from config.config import *\n",
        "from data.data_inspection import *\n",
        "from data.augmentation import *\n",
        "from data.dataset import *\n",
        "from utils.viz_utils import *\n",
        "from utils.data_utils import *\n",
        "from models.model_factory import *\n",
        "from scripts.evaluate import *\n",
        "from scripts.train import *\n",
        "from scripts.inference import predict_and_visualize_image, load_best_model\n",
        "from utils.fiftyone_utils import *\n",
        "from tuning.hyperparameters_tuning import *\n",
        "\n",
        "VIZ = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize PathsConfig\n",
        "pathconfig ={\n",
        "    \"dataset_name\": \"haldennord09\",\n",
        "    \"darwin_root\": \"/home/naro/.darwin/datasets/digital-production\",\n",
        "    \"dataset_version\": \"latest\",\n",
        "    \"extension\": 'png',\n",
        "    \"models_dir\": '/home/naro/projects/Rumex/artifacts/models'\n",
        "} \n",
        "\n",
        "paths_config = PathsConfig(**pathconfig)\n",
        "\n",
        "# Initialize DataVerifier\n",
        "data_verifier = DataVerifier(\n",
        "    img_dir=paths_config.img_dir,\n",
        "    annotations_dir=paths_config.annotations_dir,\n",
        "    train_split_file=paths_config.train_split_file,\n",
        "    test_split_file=paths_config.test_split_file,\n",
        "    val_split_file=paths_config.val_split_file,\n",
        "    extension=paths_config.extension\n",
        ")\n",
        "\n",
        "# Verify data\n",
        "train_annotations, test_annotations, val_annotations = data_verifier.check_directory_contents()\n",
        "\n",
        "# Initialize ImageProcessor\n",
        "image_processor = ImagesClassesInspector(\n",
        "    img_dir=paths_config.img_dir,\n",
        "    annotations_dir=paths_config.annotations_dir\n",
        ")\n",
        "\n",
        "# Get image and annotation lists\n",
        "train_images = data_verifier.get_image_files(train_annotations)\n",
        "val_images = data_verifier.get_image_files(val_annotations)\n",
        "test_images = data_verifier.get_image_files(test_annotations)\n",
        "\n",
        "# Get image sizes\n",
        "image_files = os.listdir(paths_config.img_dir)\n",
        "train_sizes = image_processor.get_image_sizes(image_files)\n",
        "\n",
        "# Get classes\n",
        "annotation_files = train_annotations + test_annotations + val_annotations\n",
        "classes = image_processor.get_classes(annotation_files)\n",
        "print(\"\\nClasses in the dataset:\")\n",
        "print(classes)\n",
        "\n",
        "class_map = {name: idx + 1 for idx, name in enumerate(classes)}\n",
        "print(\"\\nThe created class map:\")\n",
        "print(class_map)\n",
        "\n",
        "# Get image size stats\n",
        "min_size, max_size = image_processor.get_image_size_stats(image_files)\n",
        "print(f\"Smallest image size: {min_size}\")\n",
        "print(f\"Largest image size: {max_size}\")\n",
        "\n",
        "w_min, h_min = min_size\n",
        "print(f\"Width of smallest image: {w_min}\")\n",
        "print(f\"Height of smallest image: {h_min}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Configure the augmentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize AugmentationConfig\n",
        "augmentation_config = AugmentationConfig(height=h_min, width=w_min)\n",
        "\n",
        "# Get transforms\n",
        "train_transform = augmentation_config.get_train_transform()\n",
        "valid_transform = augmentation_config.get_valid_transform()\n",
        "\n",
        "# Print transform configurations\n",
        "print(\"Training transforms:\")\n",
        "print(train_transform)\n",
        "print(\"\\nValidation transforms:\")\n",
        "print(valid_transform)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Creating datasets and dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANbnV0-DyE7G"
      },
      "outputs": [],
      "source": [
        "train_loader, val_loader, test_loader = create_data_loaders(\n",
        "    img_dir=paths_config.img_dir,\n",
        "    annotation_dir=paths_config.annotations_dir,\n",
        "    train_images=train_images,\n",
        "    train_annotations=train_annotations,\n",
        "    val_images=val_images,\n",
        "    val_annotations=val_annotations,\n",
        "    test_images=test_images,\n",
        "    test_annotations=test_annotations,\n",
        "    train_transform=train_transform,\n",
        "    valid_transform=valid_transform,\n",
        "    class_map=class_map\n",
        ")\n",
        "\n",
        "# Print the number of samples in each dataset\n",
        "print(f\"Number of samples in training dataset: {len(train_loader.dataset)}\")\n",
        "print(f\"Number of samples in validation dataset: {len(val_loader.dataset)}\")\n",
        "print(f\"Number of samples in test dataset: {len(test_loader.dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yJDW_FosybaO",
        "outputId": "3f86f6c5-5608-4710-99d3-ee0ac5a58f11"
      },
      "outputs": [],
      "source": [
        "\n",
        "if VIZ:\n",
        "    print(\"Visualizing samples:\")\n",
        "    for i in range(3):\n",
        "        print(f\"\\nSample {i+1}:\")\n",
        "        visualize_sample(train_loader.dataset, class_map,idx=None, figsize=(5,5))\n",
        "\n",
        "    \n",
        "    # Visualize augmentations\n",
        "    print(\"Visualizing original image with augmentations:\")\n",
        "    for i in range(5):\n",
        "        visualize_augmentations(\n",
        "            dataset_without_augmentation=RumexDataSet(\n",
        "                img_dir=paths_config.img_dir,\n",
        "                annotation_dir=paths_config.annotations_dir,\n",
        "                images_list=train_images,\n",
        "                annotations_list=train_annotations,\n",
        "                transform=None,\n",
        "                class_map=class_map\n",
        "            ),\n",
        "            dataset_with_augmentation=RumexDataSet(\n",
        "                img_dir=paths_config.img_dir,\n",
        "                annotation_dir=paths_config.annotations_dir,\n",
        "                images_list=train_images,\n",
        "                annotations_list=train_annotations,\n",
        "                transform=train_transform,\n",
        "                class_map=class_map\n",
        "            ),\n",
        "            classes=classes,\n",
        "            num_augmented=5\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model creation\n",
        "\n",
        "Current possible combinations that are implemented:\n",
        "  Model Name | Model Backbone | Weights |\n",
        " |----------|----------|----------|\n",
        " | fasterrcnn    | resnet50   | COCO_V1  |\n",
        " | fasterrcnn    | mobilenet_v3_large_320   | COCO_V1  |\n",
        " | fasterrcnn    | mobilenet_v3_large   | COCO_V1  |\n",
        " | fasterrcnnV2    | resnet50   | COCO_V1  |\n",
        " | retinanet    | resnet50  | COCO_V1   |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define number of classes (update this for your dataset)\n",
        "num_classes = 2  # e.g., background + bird\n",
        "\n",
        "# Initialize a Faster R-CNN model with ResNet50 backbone\n",
        "model_config = {\n",
        "    'model_name': 'fasterrcnn',\n",
        "    'backbone_name': 'resnet50',\n",
        "    'num_classes': num_classes,\n",
        "    'device': device,\n",
        "    'weights': 'COCO_V1',\n",
        "    'train_backbone': False\n",
        "}\n",
        "\n",
        "model = init_model(**model_config)\n",
        "\n",
        "# Print model summary\n",
        "print(\"\\nModel Overview:\")\n",
        "print(f\"Model type: Faster R-CNN with ResNet50 backbone\")\n",
        "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n",
        "print(f\"Number of trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model training with default hypeparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#evaluate_map50(model, val_loader, device, iou_threshold=0.5, conf_threshold=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are two way o run mlflow (or not) and work with it.\n",
        "\n",
        "- Either I log everything to the generic server available. In this case, I have to:\n",
        "\n",
        "1- start the server the terminal mlflow server --host 127.0.0.1 --port 5000\n",
        "\n",
        "2- Set a tracking uri to the same port that was assigned to the server: mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
        "\n",
        "- Or, I do not start any tracking server, in this case the experiments default to the folder location of the code from which the\n",
        "code is executed.\n",
        "\n",
        "1- Do not assign the port 5000 as a tracking uri.\n",
        "\n",
        "2- mlflow ui --backend-store-uri /path/to/mlruns/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration\n",
        "from torchinfo import summary\n",
        "from datetime import datetime\n",
        "import mlflow\n",
        "from mlflow.models import infer_signature\n",
        " \n",
        "\n",
        "# Before running the below code, one has to run the mlflow server so that\n",
        "# it starts tracking the experiments: open a terminal\n",
        "\n",
        "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
        "version =get_dataset_version_from_darwin(pathconfig['dataset_name'], 'digital-production')\n",
        "experiment_name = pathconfig['dataset_name'] + '_V' + version +  \"_\" + model_config['model_name'] + \"_\" + model_config['backbone_name'] + \"_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "experiment_name\n",
        "\n",
        "mlflow.set_experiment(experiment_name = experiment_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "config_path = '/home/naro/projects/Rumex/config/model_config.json'\n",
        "config = load_config(config_path)\n",
        "\n",
        "with mlflow.start_run():\n",
        "    params = config\n",
        "    # Log training parameters.\n",
        "    mlflow.log_params(params)\n",
        "\n",
        "    # Log model summary.\n",
        "    with open(\"model_summary.txt\", \"w\") as f:\n",
        "        f.write(str(summary(model)))\n",
        "    mlflow.log_artifact(\"model_summary.txt\")\n",
        "\n",
        "    train_model(model, train_loader, val_loader, config, device)\n",
        "\n",
        "\n",
        "    # Save the trained model to MLflow.\n",
        "    mlflow.pytorch.log_model(model, \"model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_name = 'Saturday'\n",
        "with mlflow.start_run(run_name=run_name) as run:  # This will be the detailed run name (a child inside an experiment)\n",
        "    # Log parameters\n",
        "    mlflow.log_params({\n",
        "        \"architecture\": config['architecture'],\n",
        "        \"optimizer\": config.get('optimizer', 'sgd'),\n",
        "        \"learning_rate\": config['learning_rate'],\n",
        "        \"batch_size\": train_loader.batch_size if hasattr(train_loader, 'batch_size') else config.get('batch_size', 8),\n",
        "        \"num_epochs\": config['epochs'],\n",
        "    })\n",
        "    \n",
        "    # Get run ID for artifact paths\n",
        "    run_id = run.info.run_id\n",
        "    print(f\"MLflow Run ID: {run_id}\")\n",
        "    print(f\"Tracking URI: {mlflow.get_tracking_uri()}\")\n",
        "    \n",
        "    # Setup optimizer\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    \n",
        "    # Optimizer\n",
        "    if config.get('optimizer') == 'adam':\n",
        "        optimizer = torch.optim.Adam(params, lr=config['learning_rate'], weight_decay=0.0005)\n",
        "    else:\n",
        "        optimizer = torch.optim.SGD(params, lr=config['learning_rate'], momentum=0.9, weight_decay=0.0005)\n",
        "    \n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "    # Track best model\n",
        "    best_map_50 = 0\n",
        "    best_model_path = os.path.join(config.get('models_dir', './models'), f'best_model_{run_id}.pth')\n",
        "    \n",
        "    # Ensure models directory exists\n",
        "    os.makedirs(os.path.dirname(best_model_path), exist_ok=True)\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "    \n",
        "    # Training loop\n",
        "    for epoch in range(config['epochs']):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        batch_losses = {}\n",
        "        \n",
        "        for batch_idx, (images, targets) in enumerate(train_loader):\n",
        "            # Move to device\n",
        "            images = list(image.to(device) for image in images)\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "            \n",
        "            # Forward pass\n",
        "            loss_dict = model(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "            \n",
        "            # Track individual loss components\n",
        "            for k, v in loss_dict.items():\n",
        "                if k not in batch_losses:\n",
        "                    batch_losses[k] = 0\n",
        "                batch_losses[k] += v.item()\n",
        "            \n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += losses.item()\n",
        "            \n",
        "            # Print progress\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {losses.item():.4f}')\n",
        "        \n",
        "        # Average training loss\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        avg_batch_losses = {k: v / len(train_loader) for k, v in batch_losses.items()}\n",
        "        \n",
        "        # Validation with enhanced metrics\n",
        "        val_metrics = evaluate(model, val_loader, device, iou_thresholds=[0.5, 0.75], conf_threshold=0.5)\n",
        "        \n",
        "        # Print detailed metrics report\n",
        "        print(f\"\\nEpoch {epoch + 1}/{config['epochs']} Evaluation:\")\n",
        "        print_metrics_report(val_metrics, class_names=config.get('class_names', {1: \"Rumex\"}))\n",
        "        \n",
        "        # Update learning rate\n",
        "        lr_scheduler.step()\n",
        "        \n",
        "        # Log metrics to MLflow\n",
        "        # Main metrics\n",
        "        mlflow.log_metric(\"epoch\", epoch, step=epoch)\n",
        "        mlflow.log_metric(\"learning_rate\", optimizer.param_groups[0]['lr'], step=epoch)\n",
        "        mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
        "        \n",
        "        # Log individual loss components\n",
        "        for loss_name, loss_value in avg_batch_losses.items():\n",
        "            mlflow.log_metric(f\"train_{loss_name}\", loss_value, step=epoch)\n",
        "        \n",
        "        # mAP metrics\n",
        "        mlflow.log_metric(\"mAP\", val_metrics['mAP'], step=epoch)\n",
        "        mlflow.log_metric(\"mAP_50\", val_metrics['mAP_50'], step=epoch)\n",
        "        mlflow.log_metric(\"mAP_75\", val_metrics['mAP_75'], step=epoch)\n",
        "        \n",
        "        # Overall metrics\n",
        "        mlflow.log_metric(\"micro_precision\", val_metrics['micro_precision'], step=epoch)\n",
        "        mlflow.log_metric(\"micro_recall\", val_metrics['micro_recall'], step=epoch)\n",
        "        mlflow.log_metric(\"micro_f1\", val_metrics['micro_f1'], step=epoch)\n",
        "        mlflow.log_metric(\"macro_f1\", val_metrics['macro_f1'], step=epoch)\n",
        "        \n",
        "        # Log per-class metrics\n",
        "        for class_id, metrics in val_metrics['class_metrics'].items():\n",
        "            for metric_name, value in metrics.items():\n",
        "                if metric_name in ['precision', 'recall', 'f1']:\n",
        "                    mlflow.log_metric(f\"{class_id}_{metric_name}\", value, step=epoch)\n",
        "        \n",
        "        # Confusion matrix elements\n",
        "        mlflow.log_metric(\"total_TP\", val_metrics['total_TP'], step=epoch)\n",
        "        mlflow.log_metric(\"total_FP\", val_metrics['total_FP'], step=epoch)\n",
        "        mlflow.log_metric(\"total_FN\", val_metrics['total_FN'], step=epoch)\n",
        "        \n",
        "        # Visualization of PR curves\n",
        "        fig = visualize_pr_curves(val_metrics, class_names=config.get('class_names', {1: \"Rumex\"}))\n",
        "        \n",
        "        # Save figure to temp file and log as artifact\n",
        "        pr_curve_path = f\"pr_curve_epoch_{epoch}.png\"\n",
        "        fig.savefig(pr_curve_path)\n",
        "        mlflow.log_artifact(pr_curve_path)\n",
        "        plt.close(fig)\n",
        "        os.remove(pr_curve_path)  # Clean up temp file\n",
        "        \n",
        "        # Save best model\n",
        "        if val_metrics['mAP_50'] > best_map_50:\n",
        "            best_map_50 = val_metrics['mAP_50']\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_metrics': val_metrics,\n",
        "                'best_map_50': best_map_50,\n",
        "            }, best_model_path)\n",
        "            \n",
        "            # Log best model metrics\n",
        "            mlflow.log_metric(\"best_map_50\", best_map_50, step=epoch)\n",
        "            mlflow.log_metric(\"best_epoch\", epoch, step=epoch)\n",
        "            \n",
        "            # Log model as artifact\n",
        "            mlflow.pytorch.log_model(model, \"best_model\")\n",
        "            \n",
        "            print(f\"Saved new best model with mAP@50: {best_map_50:.4f}\")\n",
        "    \n",
        "    # Log final best model path\n",
        "    mlflow.log_param(\"best_model_path\", best_model_path)\n",
        "    \n",
        "    # Final validation metrics\n",
        "    final_metrics = evaluate(model, val_loader, device, iou_thresholds=[0.5, 0.75], conf_threshold=0.5)\n",
        "    print(\"\\nFinal Evaluation:\")\n",
        "    print_metrics_report(final_metrics, class_names=config.get('class_names', {1: \"Rumex\"}))\n",
        "    \n",
        "    # Load the best model for returning\n",
        "    checkpoint = torch.load(best_model_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Standalone Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sample inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model_path = os.path.join(paths_config.models_dir, 'best_model.pth')\n",
        "model = load_best_model(model, best_model_path)\n",
        "\n",
        "# Make predictions on multiple test samples\n",
        "print(\"Visualizing predictions on test samples:\")\n",
        "for i in range(20):  # Show 3 random samples\n",
        "    print(f\"\\nTest Sample {i + 1}:\")\n",
        "    predict_and_visualize_image(model, test_loader.dataset, device=torch.device('cuda'), confidence_threshold=0.5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inference on a big image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inference and Georeferencing for a whole flight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mirror to fiftyone\n",
        "\n",
        "- Create the dataset in fiftyone, including training, testing and validation set.\n",
        "- For a specific model, create a new fiftyone dataset with the inference - Save the inferences somewhere (for later dispatching)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file = os.path.join(paths_config.annotations_dir, annotation_files[0])\n",
        "ann = json.load(open(file))\n",
        "ann\n",
        "for ann in ann['annotations']:\n",
        "    print(ann['bounding_box'] )\n",
        "    print(ann['name'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperparameters tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hyperparameter_tuning()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rumex",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
