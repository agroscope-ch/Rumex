{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import yaml\n",
    "\n",
    "\n",
    "# For deep learning\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.ops import box_iou\n",
    "\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# For augmentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Own package imports\n",
    "os.chdir('/home/naro/projects/Rumex')\n",
    "\n",
    "from src.augmentation import *\n",
    "from src.dataset import *\n",
    "from src.model_factory import *\n",
    "\n",
    "from src.evaluate import *\n",
    "from src.train import *\n",
    "from src.inference import *\n",
    "from utils.fiftyone_utils import *\n",
    "from utils.data_inspection import *\n",
    "from utils.viz_utils import *\n",
    "from utils.data_utils import *\n",
    "from utils.generic import *\n",
    "\n",
    "VIZ = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify all pathes for the dataset are working correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"/home/naro/projects/Rumex/config/configs.yaml\"\n",
    "config = read_yaml(config_file)\n",
    "\n",
    "dataset_name = config['dataset']['dataset_name']\n",
    "darwin_root = config['dataset']['darwin_root']\n",
    "dataset_version = config['dataset']['dataset_version']\n",
    "images_extension = config['dataset']['extension']\n",
    "\n",
    "\n",
    "img_dir, annotations_dir, train_split_file, test_split_file, val_split_file = format_darwin_related_pathes(dataset_name, darwin_root, dataset_version)\n",
    "\n",
    "# Initialize DataVerifier\n",
    "data_verifier = DataVerifier(\n",
    "    img_dir = img_dir,\n",
    "    annotations_dir = annotations_dir,\n",
    "    train_split_file = train_split_file,\n",
    "    test_split_file = test_split_file,\n",
    "    val_split_file = val_split_file,\n",
    "    extension = images_extension\n",
    ")\n",
    "\n",
    "# Verify data\n",
    "train_annotations, test_annotations, val_annotations = data_verifier.check_directory_contents()\n",
    "\n",
    "# Initialize ImageProcessor\n",
    "image_processor = ImagesClassesInspector(\n",
    "    img_dir=img_dir,\n",
    "    annotations_dir=annotations_dir\n",
    ")\n",
    "\n",
    "# Get image and annotation lists\n",
    "train_images = data_verifier.get_image_files(train_annotations)\n",
    "val_images = data_verifier.get_image_files(val_annotations)\n",
    "test_images = data_verifier.get_image_files(test_annotations)\n",
    "\n",
    "# Get image sizes\n",
    "image_files = os.listdir(img_dir)\n",
    "train_sizes = image_processor.get_image_sizes(image_files)\n",
    "\n",
    "# Get classes\n",
    "annotation_files = train_annotations + test_annotations + val_annotations\n",
    "classes = image_processor.get_classes(annotation_files)\n",
    "print(\"\\nClasses in the dataset:\")\n",
    "print(classes)\n",
    "\n",
    "class_map = {name: idx + 1 for idx, name in enumerate(classes)}\n",
    "print(\"\\nThe created class map:\")\n",
    "print(class_map)\n",
    "\n",
    "# Get image size stats\n",
    "min_size, max_size = image_processor.get_image_size_stats(image_files)\n",
    "print(f\"Smallest image size: {min_size}\")\n",
    "print(f\"Largest image size: {max_size}\")\n",
    "\n",
    "w_min, h_min = min_size\n",
    "print(f\"Width of smallest image: {w_min}\")\n",
    "print(f\"Height of smallest image: {h_min}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure the augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AugmentationConfig\n",
    "augmentation_config = AugmentationConfig(height=h_min, width=w_min)\n",
    "\n",
    "# Get transforms\n",
    "train_transform = augmentation_config.get_train_transform()\n",
    "valid_transform = augmentation_config.get_valid_transform()\n",
    "\n",
    "# Print transform configurations\n",
    "print(\"Training transforms:\")\n",
    "print(train_transform)\n",
    "print(\"\\nValidation transforms:\")\n",
    "print(valid_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader, val_loader, test_loader = create_data_loaders(\n",
    "    img_dir=img_dir,\n",
    "    annotation_dir=annotations_dir,\n",
    "    train_images=train_images,\n",
    "    train_annotations=train_annotations,\n",
    "    val_images=val_images,\n",
    "    val_annotations=val_annotations,\n",
    "    test_images=test_images,\n",
    "    test_annotations=test_annotations,\n",
    "    train_transform=train_transform,\n",
    "    valid_transform=valid_transform,\n",
    "    class_map=class_map,\n",
    "    batch_size=8, \n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# Print the number of samples in each dataset\n",
    "print(f\"Number of samples in training dataset: {len(train_loader.dataset)}\")\n",
    "print(f\"Number of samples in validation dataset: {len(val_loader.dataset)}\")\n",
    "print(f\"Number of samples in test dataset: {len(test_loader.dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if VIZ:\n",
    "    print(\"Visualizing samples:\")\n",
    "    for i in range(3):\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        visualize_sample(train_loader.dataset, class_map,idx=None, figsize=(5,5))\n",
    "\n",
    "    \n",
    "    # Visualize augmentations\n",
    "    print(\"Visualizing original image with augmentations:\")\n",
    "    for i in range(5):\n",
    "        visualize_augmentations(\n",
    "            dataset_without_augmentation=RumexDataSet(\n",
    "                img_dir=img_dir,\n",
    "                annotation_dir=annotations_dir,\n",
    "                images_list=train_images,\n",
    "                annotations_list=train_annotations,\n",
    "                transform=None,\n",
    "                class_map=class_map\n",
    "            ),\n",
    "            dataset_with_augmentation=RumexDataSet(\n",
    "                img_dir=img_dir,\n",
    "                annotation_dir=annotations_dir,\n",
    "                images_list=train_images,\n",
    "                annotations_list=train_annotations,\n",
    "                transform=train_transform,\n",
    "                class_map=class_map\n",
    "            ),\n",
    "            classes=classes,\n",
    "            num_augmented=5\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = init_model(model_name= config['model']['model_name'],\n",
    "                   backbone_name=config['model']['backbone'],\n",
    "                   num_classes=config['model']['num_classes'],\n",
    "                   device=device,\n",
    "                   weights=config['model']['weights'],\n",
    "                   train_backbone=config['model']['train_backbone'])\n",
    "\n",
    "# Print model summary\n",
    "print(\"\\nModel Overview:\")\n",
    "print(f\"Model type: Faster R-CNN with ResNet50 backbone\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"Number of trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Setup\n",
    "\n",
    "There are two way o run mlflow (or not) and work with it.\n",
    "\n",
    "- Either I log everything to the generic server available. In this case, I have to:\n",
    "\n",
    "1- start the server the terminal mlflow server --host 127.0.0.1 --port 5000\n",
    "\n",
    "2- Set a tracking uri to the same port that was assigned to the server: mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "- Or, I do not start any tracking server, in this case the experiments default to the folder location of the code from which the\n",
    "code is executed.\n",
    "\n",
    "1- Do not assign the port 5000 as a tracking uri.\n",
    "\n",
    "2- mlflow ui --backend-store-uri /path/to/mlruns/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "from datetime import datetime\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    " \n",
    "\n",
    "# Before running the below code, one has to run the mlflow server so that\n",
    "# it starts tracking the experiments: open a terminal\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "version =get_dataset_version_from_darwin(dataset_name, 'digital-production')\n",
    "darwin_root = config['dataset']['darwin_root']\n",
    "dataset_version = config['dataset']['dataset_version']\n",
    "images_extension = config['dataset']['extension']\n",
    "\n",
    "experiment_name = dataset_name + '_V' + version +  \"_\" + config['model']['model_name'] + \"_\" + config['model']['backbone'] + \"_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "experiment_name\n",
    "\n",
    "mlflow.set_experiment(experiment_name = experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = config['model'] \n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = False\n",
    "if TRAIN:\n",
    "    with mlflow.start_run():\n",
    "        # Log training parameters.\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        # Log model summary.\n",
    "        with open(\"model_summary.txt\", \"w\") as f:\n",
    "            f.write(str(summary(model)))\n",
    "        mlflow.log_artifact(\"model_summary.txt\")\n",
    "\n",
    "        train_model(model, train_loader, val_loader, config['model'] , device)\n",
    "\n",
    "\n",
    "        # Save the trained model to MLflow.\n",
    "        mlflow.pytorch.log_model(model, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_best_model(model = model,\n",
    "                        best_model_path= \"/home/naro/projects/Rumex/artifacts/models/best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_and_visualize_image_from_dataloader(model, test_loader, idx=1, device = torch.device('cuda'), confidence_threshold=0.5, figsize=(12, 12))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
