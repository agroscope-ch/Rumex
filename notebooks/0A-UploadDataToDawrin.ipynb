{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the necessary modules\n",
    "import os\n",
    "import darwin\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import darwin.importer as importer\n",
    "from darwin.client import Client\n",
    "from darwin.importer import get_importer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the connection with Darwin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the connection\n",
    "from darwin.client import Client\n",
    "\n",
    "with open('../keys.json') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "API_KEY = data['darwin_api_key']\n",
    "client = Client.from_api_key(API_KEY)\n",
    "\n",
    "datasets = client.list_remote_datasets()\n",
    "\n",
    "# Print dataset names\n",
    "for dataset in datasets:\n",
    "    print(dataset.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_images_darwin_dataset(dataset_slug):\n",
    "    remote_dataset = client.get_remote_dataset(dataset_slug.lower().replace('.', '-'))\n",
    "    a = remote_dataset.fetch_remote_files()\n",
    "    list_files =[s.filename for s in a]\n",
    "    return list_files\n",
    "\n",
    "def clean_dataset_slug(text):\n",
    "    replacements = {\n",
    "        'ä': 'a', 'ö': 'o', 'ü': 'u', 'ß': 'ss',\n",
    "        'Ä': 'A', 'Ö': 'O', 'Ü': 'U'\n",
    "    }\n",
    "    \n",
    "    for german_char, replacement in replacements.items():\n",
    "        text = text.replace(german_char, replacement)\n",
    "    \n",
    "    return text.lower().replace('.', '-')\n",
    "\n",
    "# Example\n",
    "original = \"20241203_Löhre_Tänikon_S_30_F_70_H_12_O_krma_ID2\"\n",
    "result = clean_dataset_slug(original)\n",
    "print(result)\n",
    "# Output: 20241203_Lohre_Tanikon_S_30_F_70_H_12_O_krma_ID2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/data/images/rumex/Temp\"\n",
    "for dataset_slug in os.listdir(root):\n",
    "    list_images_local = os.listdir(f'/data/images/rumex/Temp/{dataset_slug}')\n",
    "    # pick only the images with the following format png PNG JPG jpg JPEG jpeg\n",
    "    list_images_local = [image for image in list_images_local if image.split('.')[-1].lower() in ['png', 'jpg', 'jpeg']]\n",
    "\n",
    "    # Add images to a Path list\n",
    "    images = [Path(os.path.join(root, dataset_slug, i)) for i in list_images_local]\n",
    "    \n",
    "    try:\n",
    "        remote_dataset = client.get_remote_dataset(clean_dataset_slug(dataset_slug))\n",
    "        print(f\"Dataset '{dataset_slug}' exists.\")\n",
    "\n",
    "        # The dataset exists, check if the images are already uploaded\n",
    "        list_images_remote= remote_dataset.fetch_remote_files()\n",
    "        list_images_remote =[s.filename for s in list_images_remote]\n",
    "\n",
    "        list_images_on_local_but_not_remote = list(set(list_images_local) - set(list_images_remote))\n",
    "\n",
    "        \n",
    "        if len(list_images_on_local_but_not_remote) == 0:\n",
    "            print(f\"No images uploaded in dataset '{dataset_slug}'\")\n",
    "        else:\n",
    "            print(f\"Uploading {len(list_images_on_local_but_not_remote)} images to dataset '{dataset_slug}'\")\n",
    "            print(list_images_on_local_but_not_remote)\n",
    "            images_to_upload = [Path(os.path.join(root, dataset_slug, i)) for i in list_images_on_local_but_not_remote]\n",
    "            handler = remote_dataset.push(images_to_upload)\n",
    "\n",
    "\n",
    "    except Exception:\n",
    "        print(f\"Dataset '{dataset_slug}' does not exist - Creating a new one\")\n",
    "        dataset = client.create_dataset(clean_dataset_slug(dataset_slug))\n",
    "        # Upload images to the dataset\n",
    "        handler = dataset.push(images) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading tiled datasets (annotations only. Images have been uploaded using the interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = (\n",
    "    \"/mnt/Data-Work-RE/26_Agricultural_Engineering-RE/263_DP/00_Darwin/digital-production\"\n",
    ")\n",
    "\n",
    "for dataset_slug in ['haldennord10']: #lightly, bildacher\n",
    "\n",
    "    images_dir = os.path.join(root, dataset_slug, 'images_splitted')\n",
    "    print(images_dir)\n",
    "    list_images = os.listdir(images_dir)\n",
    "    list_images = [image for image in list_images if image.split('.')[-1].lower() in ['png', 'jpg', 'jpeg']]\n",
    "    \n",
    "    annotations_dir = os.path.join(root, dataset_slug, 'releases/1/annotations_splitted')\n",
    "    list_annotations = os.listdir(annotations_dir)\n",
    "    list_annotations = [ann for ann in list_annotations if ann.split('.')[-1].lower() == 'json']\n",
    "    \n",
    "    # Add images to a Path list\n",
    "    images = [Path(os.path.join(root, dataset_slug, 'images_splitted', i)) for i in list_images]\n",
    "    annotations = [Path(os.path.join(root, dataset_slug, 'releases/1/annotations_splitted', i)) for i in list_annotations]\n",
    "    \n",
    "    # annotations_new = sorted(annotations)\n",
    "    # print(len(annotations_new))\n",
    "    # to_upload = annotations_new[50:100]\n",
    "    # print('/n/n/n')\n",
    "    # print(annotations_new[0:5])\n",
    "    # print('/n/n/n')\n",
    "    dataset = client.get_remote_dataset(clean_dataset_slug(dataset_slug+'splitted'))\n",
    "    print(f\"Dataset '{dataset_slug+'splitted'}' exists.\")\n",
    "    parser = get_importer('darwin')\n",
    "    \n",
    "    batch_size = 50\n",
    "    for i in range(0, len(annotations), batch_size):\n",
    "        to_upload = annotations[i:i + batch_size]\n",
    "        \n",
    "        # You can now use `to_upload` for processing/uploading\n",
    "        print(f\"Uploading batch {i // batch_size + 1}: {len(to_upload)} items\")\n",
    "        importer.import_annotations(dataset, parser, to_upload, append=False, overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Computer Vision Default",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
