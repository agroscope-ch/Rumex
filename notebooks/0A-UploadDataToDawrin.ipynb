{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the necessary modules\n",
    "import os\n",
    "import darwin\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import darwin.importer as importer\n",
    "from darwin.client import Client\n",
    "from darwin.importer import get_importer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the connection with Darwin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the connection\n",
    "from darwin.client import Client\n",
    "\n",
    "with open('../keys.json') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "API_KEY = data['darwin_api_key']\n",
    "client = Client.from_api_key(API_KEY)\n",
    "\n",
    "datasets = client.list_remote_datasets()\n",
    "\n",
    "# Print dataset names\n",
    "for dataset in datasets:\n",
    "    print(dataset.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_slug = \"20220413_Bilder_Matrice_300_Willi_Hoenggerberg_Zuerich\"\n",
    "remote_dataset = client.get_remote_dataset(dataset_slug.lower())\n",
    "a = remote_dataset.fetch_remote_files()\n",
    "list_files =[s.filename for s in a]\n",
    "print(list_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/data/images/rumex/Temp\"\n",
    "for dataset_slug in os.listdir(root):\n",
    "    print(dataset_slug)\n",
    "    list_images = os.listdir(f'/data/images/rumex/Temp/{dataset_slug}')\n",
    "    # pick only the images with the following format png PNG JPG jpg JPEG jpeg\n",
    "    list_images = [image for image in list_images if image.split('.')[-1].lower() in ['png', 'jpg', 'jpeg']]\n",
    "\n",
    "    # Add images to a Path list\n",
    "    images = [Path(os.path.join(root, dataset_slug, i)) for i in list_images]\n",
    "    \n",
    "    try:\n",
    "        remote_dataset = client.get_remote_dataset(dataset_slug.lower())\n",
    "        print(f\"Dataset '{dataset_slug}' exists.\")\n",
    "\n",
    "\n",
    "    except Exception:\n",
    "        print(f\"Dataset '{dataset_slug}' does not exist - Creating a new one\")\n",
    "    \n",
    "        dataset = client.create_dataset(dataset_slug)\n",
    "\n",
    "\n",
    "        # Upload images to the dataset\n",
    "        handler = dataset.push(images) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darwin.client import Client\n",
    "\n",
    "client = Client.local()\n",
    "dataset_identifier = \"your-team/your-dataset\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "image_filename = \"your-image.jpg\"\n",
    "dataset_files = remote_dataset.files()\n",
    "\n",
    "if any(file.name == image_filename for file in dataset_files):\n",
    "    print(f\"Image '{image_filename}' exists in the dataset.\")\n",
    "else:\n",
    "    print(f\"Image '{image_filename}' does not exist in the dataset.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting the database in shape for the upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the path provided by Ralph\n",
    "path = \"/mnt/Foto-Work-RE/26_Agricultural_Engineering-RE/263_DP/Fenaco_Blackenprojekt_2021-2023/20250304DatasetSummaryForAnnotations.txt\"\n",
    "\n",
    "# Read the text file in a pandas dataframe\n",
    "df = pd.read_csv(path, sep = '\\s+')\n",
    "\n",
    "\n",
    "def process_row(row):\n",
    "    # Do something with the row\n",
    "    return row['column_a'] + row['column_b']\n",
    "\n",
    "def process_row(row, original_root, new_root):\n",
    "    \"\"\"\n",
    "    Replace the original path root with the new path root in a DataFrame row.\n",
    "    \n",
    "    Parameters:\n",
    "    row: pandas Series (a single row from the DataFrame)\n",
    "    original_root: str, the Windows path root to replace\n",
    "    new_root: str, the Unix path root to use instead\n",
    "    \n",
    "    Returns:\n",
    "    pandas Series with updated paths\n",
    "    \"\"\"\n",
    "    # Create a copy of the row to avoid SettingWithCopyWarning\n",
    "    updated_row = row.copy()\n",
    "    \n",
    "    # Loop through all columns in the row\n",
    "    for column in row.index:\n",
    "        if isinstance(row[column], str) and original_root in row[column]:\n",
    "            # Replace backslashes with forward slashes first\n",
    "            temp_path = row[column].replace('\\\\', '/')\n",
    "            # Then replace the root part\n",
    "            original_root_forward = original_root.replace('\\\\', '/')\n",
    "            updated_row[column] = temp_path.replace(original_root_forward, new_root)\n",
    "    \n",
    "    return updated_row\n",
    "\n",
    "# Example usage:\n",
    "original_root = 'O:\\\\Foto-Video-Research\\\\26_Agricultural_Engineering-RE\\\\263_DP\\\\Fenaco_Blackenprojekt_2021-2023'\n",
    "new_root = '/mnt/Foto-Work-RE/26_Agricultural_Engineering-RE/263_DP/Fenaco_Blackenprojekt_2021-2023'\n",
    "\n",
    "# Apply to the entire DataFrame\n",
    "df_updated = df.apply(lambda row: process_row(row, original_root, new_root), axis=1)\n",
    "\n",
    "dataset_names = [i.split('/')[6] for i in df_updated['Path']]\n",
    "dataset_names = set(dataset_names)\n",
    "dataset_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(dataset_names)} Datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = list(dataset_names)\n",
    "\n",
    "filtered_df = df_updated[df_updated['Path'].str.contains(dataset_names[0])]\n",
    "filtered_df\n",
    "\n",
    "dataset_names = list(dataset_names)\n",
    "dataset_slug = dataset_names[0]\n",
    "dataset_slug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one test dataset\n",
    "dataset = client.create_dataset(dataset_slug)\n",
    "print(dataset.slug)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(filtered_df['Path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all the datasets\n",
    "files = [Path(i) for i in filtered_df['Path']]\n",
    "handler = dataset.push(files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add a mechanism to check for datasets that exists already so that we do not create a new dataset for it. Also, add a mechanism to do the same for individual pictures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filtered_df['Path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(handler.pending_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def darwin_uploader(dataset_name):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to tag the items\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "url = f\"https://darwin.v7labs.com/api/v2/teams/{team-slug}/items/slots/tags\"\n",
    "\n",
    "\n",
    "payload = {\n",
    "    \"filters\": {\n",
    "        \"dataset_ids\": [1651025],\n",
    "        \"item_name_contains\": \"item-name.jpg\"\n",
    "    },\n",
    "    \"annotation_class_id\": 430588\n",
    "}\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"content-type\": \"application/json\",\n",
    "    \"Authorization\": \"ApiKey {API-KEY}\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rumex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
